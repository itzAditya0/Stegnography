{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":502445,"sourceType":"datasetVersion","datasetId":236239},{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":2307647,"sourceType":"datasetVersion","datasetId":1391879},{"sourceId":4043617,"sourceType":"datasetVersion","datasetId":2395063},{"sourceId":5030375,"sourceType":"datasetVersion","datasetId":2919327}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet-pytorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:19.935817Z","iopub.execute_input":"2025-01-11T18:18:19.936136Z","iopub.status.idle":"2025-01-11T18:18:22.995961Z","shell.execute_reply.started":"2025-01-11T18:18:19.936113Z","shell.execute_reply":"2025-01-11T18:18:22.994843Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet-pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision import datasets, transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom efficientnet_pytorch import EfficientNet\nimport numpy as np\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.datasets import FashionMNIST","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:22.997369Z","iopub.execute_input":"2025-01-11T18:18:22.997736Z","iopub.status.idle":"2025-01-11T18:18:23.002276Z","shell.execute_reply.started":"2025-01-11T18:18:22.997710Z","shell.execute_reply":"2025-01-11T18:18:23.001484Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Define Transformation\n\ndef get_transform():\n    return transforms.Compose([\n        transforms.Resize((128, 128)),  # Resize all images to 128x128\n        transforms.Grayscale(num_output_channels=3),  # Convert grayscale images to 3 channels\n        transforms.ToTensor(),  # Convert images to tensor\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images for 3 channels\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:23.004270Z","iopub.execute_input":"2025-01-11T18:18:23.004521Z","iopub.status.idle":"2025-01-11T18:18:23.033788Z","shell.execute_reply.started":"2025-01-11T18:18:23.004502Z","shell.execute_reply":"2025-01-11T18:18:23.033085Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Example for FashionMNIST dataset\nfrom torchvision.datasets import FashionMNIST\n\nfashion_mnist_train = FashionMNIST(root='data', train=True, download=True, transform=get_transform())\nfashion_mnist_test = FashionMNIST(root='data', train=False, download=True, transform=get_transform())\n\n# You can apply similar code to the other datasets.\n\ntransform = get_transform()\n\n# Load CIFAR-10 dataset\ncifar10_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ncifar10_test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# Load FashionMNIST dataset\nfashionmnist_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\nfashionmnist_test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n# Load StegoImages dataset\nfrom torchvision.datasets import ImageFolder\nstego_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/train', transform=transform)\nstego_test_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/test', transform=transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:23.034769Z","iopub.execute_input":"2025-01-11T18:18:23.035044Z","iopub.status.idle":"2025-01-11T18:18:44.432213Z","shell.execute_reply.started":"2025-01-11T18:18:23.035024Z","shell.execute_reply":"2025-01-11T18:18:44.431548Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"combined_train_data = ConcatDataset([cifar10_data, fashionmnist_data, stego_data])\ncombined_test_data = ConcatDataset([cifar10_test_data, fashionmnist_test_data, stego_test_data])\n\ntrain_loader = DataLoader(combined_train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(combined_test_data, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.432880Z","iopub.execute_input":"2025-01-11T18:18:44.433083Z","iopub.status.idle":"2025-01-11T18:18:44.443845Z","shell.execute_reply.started":"2025-01-11T18:18:44.433065Z","shell.execute_reply":"2025-01-11T18:18:44.442849Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet\n\nclass StegoLocationNet(nn.Module):\n    def __init__(self, num_classes):\n        super(StegoLocationNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Load EfficientNet base model\n        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n\n        # Retrieve the in_features from the original _fc layer\n        if hasattr(self.base_model, '_fc') and isinstance(self.base_model._fc, nn.Linear):\n            in_features = self.base_model._fc.in_features\n        else:\n            raise AttributeError(\"EfficientNet model does not have a valid '_fc' layer.\")\n\n        # Replace the classification layer\n        self.base_model._fc = nn.Identity()  # Remove the existing fully connected layer\n\n        # Add custom classification head\n        self.classifier = nn.Linear(in_features, num_classes)\n\n        # Add location detection layers\n        self.location_head = nn.Sequential(\n            nn.Conv2d(in_features, 128, kernel_size=1),  # Match input channels to 1280\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Extract features before the final classifier\n        features = self.base_model.extract_features(x)\n\n        # Generate location map\n        location_map = self.location_head(features)\n\n        # Optional classification (not used for location-only tasks)\n        pooled_features = torch.mean(features, dim=[2, 3])  # Global average pooling\n        class_output = self.classifier(pooled_features)\n\n        return location_map, class_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.444728Z","iopub.execute_input":"2025-01-11T18:18:44.445015Z","iopub.status.idle":"2025-01-11T18:18:44.464334Z","shell.execute_reply.started":"2025-01-11T18:18:44.444983Z","shell.execute_reply":"2025-01-11T18:18:44.463706Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def generate_hiding_spots(images):\n    # Example heuristic: higher values in uniform regions\n    # Apply Sobel filter to find edges\n    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3).float().to(images.device)\n    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3).float().to(images.device)\n\n    edges_x = F.conv2d(images, sobel_x, padding=1)\n    edges_y = F.conv2d(images, sobel_y, padding=1)\n    edge_magnitude = torch.sqrt(edges_x**2 + edges_y**2)\n\n    # Normalize and invert edge magnitude to prioritize smooth regions\n    suitability = 1 - (edge_magnitude / edge_magnitude.max())\n    return suitability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.464895Z","iopub.execute_input":"2025-01-11T18:18:44.465099Z","iopub.status.idle":"2025-01-11T18:18:44.482109Z","shell.execute_reply.started":"2025-01-11T18:18:44.465081Z","shell.execute_reply":"2025-01-11T18:18:44.481402Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"num_classes = 10  # Update based on your dataset\nmodel = StegoLocationNet(num_classes=num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.482767Z","iopub.execute_input":"2025-01-11T18:18:44.482953Z","iopub.status.idle":"2025-01-11T18:18:44.589948Z","shell.execute_reply.started":"2025-01-11T18:18:44.482937Z","shell.execute_reply":"2025-01-11T18:18:44.589175Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\n# Get the number of classes from your dataset\nnum_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n\n# Load the EfficientNet model\n# Get the number of classes from your dataset\nnum_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n\n# Create model\nmodel = StegoLocationNet(num_classes=num_classes)\n\n# Move model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.592226Z","iopub.execute_input":"2025-01-11T18:18:44.592428Z","iopub.status.idle":"2025-01-11T18:18:44.692791Z","shell.execute_reply.started":"2025-01-11T18:18:44.592411Z","shell.execute_reply":"2025-01-11T18:18:44.691946Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.693994Z","iopub.execute_input":"2025-01-11T18:18:44.694307Z","iopub.status.idle":"2025-01-11T18:18:44.700442Z","shell.execute_reply.started":"2025-01-11T18:18:44.694275Z","shell.execute_reply":"2025-01-11T18:18:44.699761Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Add this helper function above the training loop\ndef generate_hiding_spots(images, threshold=0.8):\n    \"\"\"\n    Generate target masks indicating good hiding spots\n    This is a simple example using edge detection and local variance\n    \"\"\"\n    batch_size = images.size(0)\n    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n    \n    for i in range(batch_size):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        \n        # Convert to grayscale\n        gray = np.mean(img, axis=2)\n        \n        # Calculate local variance\n        local_var = ndimage.generic_filter(gray, np.var, size=3)\n        \n        # Normalize and threshold\n        local_var = (local_var - local_var.min()) / (local_var.max() - local_var.min())\n        masks[i, 0] = torch.from_numpy(local_var > threshold).float().to(images.device)\n    \n    return masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.701479Z","iopub.execute_input":"2025-01-11T18:18:44.701788Z","iopub.status.idle":"2025-01-11T18:18:44.712970Z","shell.execute_reply.started":"2025-01-11T18:18:44.701750Z","shell.execute_reply":"2025-01-11T18:18:44.712271Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Define the function to generate target masks\ndef generate_hiding_spots(images, threshold=0.8):\n    \"\"\"\n    Generate target masks indicating good hiding spots.\n    This is a simple example using edge detection and local variance.\n    \"\"\"\n    batch_size = images.size(0)\n    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n\n    # Example implementation: Fill the masks with zeros or apply a heuristic\n    for i in range(batch_size):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        # Add your heuristic for detecting hiding spots here\n        # For example, based on edge detection, variance, or other methods\n        pass\n\n    return masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.713589Z","iopub.execute_input":"2025-01-11T18:18:44.713871Z","iopub.status.idle":"2025-01-11T18:18:44.729990Z","shell.execute_reply.started":"2025-01-11T18:18:44.713852Z","shell.execute_reply":"2025-01-11T18:18:44.729189Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 2\nbatch_size = 64  # Define the batch size for DataLoader\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0  # Track the total loss for the epoch\n\n    for i, (images, _) in enumerate(train_loader):  # Process in batches\n        images = images.to(device)\n\n        # Generate target masks\n        target_masks = generate_hiding_spots(images)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        location_maps, _ = model(images)  # Unpack the tuple to get location_map\n\n        # Upsample location_maps to match target_masks\n        location_maps_upsampled = torch.nn.functional.interpolate(\n            location_maps, size=target_masks.shape[2:], mode='bilinear', align_corners=False\n        )\n\n        # Calculate loss\n        loss = criterion(location_maps_upsampled, target_masks)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()  # Accumulate loss for the epoch\n\n    # Log average loss for the epoch\n    avg_loss = epoch_loss / len(train_loader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:18:44.730664Z","iopub.execute_input":"2025-01-11T18:18:44.730888Z","iopub.status.idle":"2025-01-11T18:34:54.681335Z","shell.execute_reply.started":"2025-01-11T18:18:44.730870Z","shell.execute_reply":"2025-01-11T18:34:54.680481Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/2], Average Loss: 0.0090\nEpoch [2/2], Average Loss: 0.0001\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"print(f\"Location maps size: {location_maps.shape}\")\nprint(f\"Target masks size: {target_masks.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:34:54.682282Z","iopub.execute_input":"2025-01-11T18:34:54.682599Z","iopub.status.idle":"2025-01-11T18:34:54.687357Z","shell.execute_reply.started":"2025-01-11T18:34:54.682568Z","shell.execute_reply":"2025-01-11T18:34:54.686529Z"}},"outputs":[{"name":"stdout","text":"Location maps size: torch.Size([48, 1, 4, 4])\nTarget masks size: torch.Size([48, 1, 128, 128])\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0.0\n    with torch.no_grad():\n        for images, _ in test_loader:  # Ignore original labels\n            images = images.to(device)\n\n            # Generate target masks for testing\n            target_masks = generate_hiding_spots(images)\n\n            # Forward pass\n            location_maps, _ = model(images)\n\n            # Resize `location_maps` to match `target_masks` size\n            location_maps_resized = F.interpolate(\n                location_maps, size=target_masks.shape[2:], mode=\"bilinear\", align_corners=False\n            )\n\n            # Compute loss\n            loss = criterion(location_maps_resized, target_masks)\n\n            # Accumulate loss\n            test_loss += loss.item()\n\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n    return avg_test_loss\n\n# Perform evaluation\naverage_test_loss = evaluate_model(model, test_loader, criterion, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T18:34:54.688078Z","iopub.execute_input":"2025-01-11T18:34:54.688293Z","iopub.status.idle":"2025-01-11T18:37:41.271657Z","shell.execute_reply.started":"2025-01-11T18:34:54.688274Z","shell.execute_reply":"2025-01-11T18:37:41.270738Z"}},"outputs":[{"name":"stdout","text":"Average Test Loss: 0.0000\n","output_type":"stream"}],"execution_count":35}]}