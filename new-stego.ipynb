{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6f5b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:24:20.399920Z",
     "iopub.status.busy": "2025-01-11T17:24:20.399628Z",
     "iopub.status.idle": "2025-01-11T17:24:28.536064Z",
     "shell.execute_reply": "2025-01-11T17:24:28.535170Z"
    },
    "papermill": {
     "duration": 8.143243,
     "end_time": "2025-01-11T17:24:28.537691",
     "exception": false,
     "start_time": "2025-01-11T17:24:20.394448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.4.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=3ad06cf090bd760e1cadbdfb45aaa6156b26542fc4d3ab337275bb540f9a34a4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "Successfully built efficientnet-pytorch\r\n",
      "Installing collected packages: efficientnet-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5855a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:24:28.545695Z",
     "iopub.status.busy": "2025-01-11T17:24:28.545433Z",
     "iopub.status.idle": "2025-01-11T17:24:36.980042Z",
     "shell.execute_reply": "2025-01-11T17:24:36.979351Z"
    },
    "papermill": {
     "duration": 8.44025,
     "end_time": "2025-01-11T17:24:36.981634",
     "exception": false,
     "start_time": "2025-01-11T17:24:28.541384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3266899d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:24:36.989437Z",
     "iopub.status.busy": "2025-01-11T17:24:36.989080Z",
     "iopub.status.idle": "2025-01-11T17:24:36.993162Z",
     "shell.execute_reply": "2025-01-11T17:24:36.992304Z"
    },
    "papermill": {
     "duration": 0.009246,
     "end_time": "2025-01-11T17:24:36.994526",
     "exception": false,
     "start_time": "2025-01-11T17:24:36.985280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Transformation\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Resize all images to 128x128\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert grayscale images to 3 channels\n",
    "        transforms.ToTensor(),  # Convert images to tensor\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images for 3 channels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77830d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:24:37.001477Z",
     "iopub.status.busy": "2025-01-11T17:24:37.001232Z",
     "iopub.status.idle": "2025-01-11T17:26:04.401302Z",
     "shell.execute_reply": "2025-01-11T17:26:04.400325Z"
    },
    "papermill": {
     "duration": 87.405539,
     "end_time": "2025-01-11T17:26:04.403137",
     "exception": false,
     "start_time": "2025-01-11T17:24:36.997598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 18143890.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 264337.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 4928901.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 21832433.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 104523791.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Example for FashionMNIST dataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "fashion_mnist_train = FashionMNIST(root='data', train=True, download=True, transform=get_transform())\n",
    "fashion_mnist_test = FashionMNIST(root='data', train=False, download=True, transform=get_transform())\n",
    "\n",
    "# You can apply similar code to the other datasets.\n",
    "\n",
    "transform = get_transform()\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar10_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar10_test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashionmnist_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "fashionmnist_test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Load StegoImages dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "stego_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/train', transform=transform)\n",
    "stego_test_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/test', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fd3acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:04.416852Z",
     "iopub.status.busy": "2025-01-11T17:26:04.416609Z",
     "iopub.status.idle": "2025-01-11T17:26:04.420848Z",
     "shell.execute_reply": "2025-01-11T17:26:04.420169Z"
    },
    "papermill": {
     "duration": 0.012153,
     "end_time": "2025-01-11T17:26:04.422040",
     "exception": false,
     "start_time": "2025-01-11T17:26:04.409887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_train_data = ConcatDataset([cifar10_data, fashionmnist_data, stego_data])\n",
    "combined_test_data = ConcatDataset([cifar10_test_data, fashionmnist_test_data, stego_test_data])\n",
    "\n",
    "train_loader = DataLoader(combined_train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(combined_test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f7a0e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:04.434779Z",
     "iopub.status.busy": "2025-01-11T17:26:04.434576Z",
     "iopub.status.idle": "2025-01-11T17:26:04.440101Z",
     "shell.execute_reply": "2025-01-11T17:26:04.439546Z"
    },
    "papermill": {
     "duration": 0.013122,
     "end_time": "2025-01-11T17:26:04.441220",
     "exception": false,
     "start_time": "2025-01-11T17:26:04.428098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class StegoLocationNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(StegoLocationNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Load EfficientNet base model\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "        # Retrieve the in_features from the original _fc layer\n",
    "        if hasattr(self.base_model, '_fc') and isinstance(self.base_model._fc, nn.Linear):\n",
    "            in_features = self.base_model._fc.in_features\n",
    "        else:\n",
    "            raise AttributeError(\"EfficientNet model does not have a valid '_fc' layer.\")\n",
    "\n",
    "        # Replace the classification layer\n",
    "        self.base_model._fc = nn.Identity()  # Remove the existing fully connected layer\n",
    "\n",
    "        # Add custom classification head\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        # Add location detection layers\n",
    "        self.location_head = nn.Sequential(\n",
    "            nn.Conv2d(in_features, 128, kernel_size=1),  # Match input channels to 1280\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features before the final classifier\n",
    "        features = self.base_model.extract_features(x)\n",
    "\n",
    "        # Generate location map\n",
    "        location_map = self.location_head(features)\n",
    "\n",
    "        # Optional classification (not used for location-only tasks)\n",
    "        pooled_features = torch.mean(features, dim=[2, 3])  # Global average pooling\n",
    "        class_output = self.classifier(pooled_features)\n",
    "\n",
    "        return location_map, class_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b428ef15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:04.453734Z",
     "iopub.status.busy": "2025-01-11T17:26:04.453534Z",
     "iopub.status.idle": "2025-01-11T17:26:04.457761Z",
     "shell.execute_reply": "2025-01-11T17:26:04.457160Z"
    },
    "papermill": {
     "duration": 0.011715,
     "end_time": "2025-01-11T17:26:04.458890",
     "exception": false,
     "start_time": "2025-01-11T17:26:04.447175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_hiding_spots(images):\n",
    "    # Example heuristic: higher values in uniform regions\n",
    "    # Apply Sobel filter to find edges\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3).float().to(images.device)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3).float().to(images.device)\n",
    "\n",
    "    edges_x = F.conv2d(images, sobel_x, padding=1)\n",
    "    edges_y = F.conv2d(images, sobel_y, padding=1)\n",
    "    edge_magnitude = torch.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "    # Normalize and invert edge magnitude to prioritize smooth regions\n",
    "    suitability = 1 - (edge_magnitude / edge_magnitude.max())\n",
    "    return suitability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe9652b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:04.471306Z",
     "iopub.status.busy": "2025-01-11T17:26:04.471064Z",
     "iopub.status.idle": "2025-01-11T17:26:05.079579Z",
     "shell.execute_reply": "2025-01-11T17:26:05.078687Z"
    },
    "papermill": {
     "duration": 0.616115,
     "end_time": "2025-01-11T17:26:05.080914",
     "exception": false,
     "start_time": "2025-01-11T17:26:04.464799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:00<00:00, 150MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10  # Update based on your dataset\n",
    "model = StegoLocationNet(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bde4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:05.095199Z",
     "iopub.status.busy": "2025-01-11T17:26:05.094975Z",
     "iopub.status.idle": "2025-01-11T17:26:05.507461Z",
     "shell.execute_reply": "2025-01-11T17:26:05.506487Z"
    },
    "papermill": {
     "duration": 0.421,
     "end_time": "2025-01-11T17:26:05.509033",
     "exception": false,
     "start_time": "2025-01-11T17:26:05.088033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the number of classes from your dataset\n",
    "num_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n",
    "\n",
    "# Load the EfficientNet model\n",
    "# Get the number of classes from your dataset\n",
    "num_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n",
    "\n",
    "# Create model\n",
    "model = StegoLocationNet(num_classes=num_classes)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ae1a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:05.522906Z",
     "iopub.status.busy": "2025-01-11T17:26:05.522662Z",
     "iopub.status.idle": "2025-01-11T17:26:05.527292Z",
     "shell.execute_reply": "2025-01-11T17:26:05.526553Z"
    },
    "papermill": {
     "duration": 0.012956,
     "end_time": "2025-01-11T17:26:05.528712",
     "exception": false,
     "start_time": "2025-01-11T17:26:05.515756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d61cbeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:05.542357Z",
     "iopub.status.busy": "2025-01-11T17:26:05.542073Z",
     "iopub.status.idle": "2025-01-11T17:26:05.547121Z",
     "shell.execute_reply": "2025-01-11T17:26:05.546455Z"
    },
    "papermill": {
     "duration": 0.013216,
     "end_time": "2025-01-11T17:26:05.548348",
     "exception": false,
     "start_time": "2025-01-11T17:26:05.535132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add this helper function above the training loop\n",
    "def generate_hiding_spots(images, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Generate target masks indicating good hiding spots\n",
    "    This is a simple example using edge detection and local variance\n",
    "    \"\"\"\n",
    "    batch_size = images.size(0)\n",
    "    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = np.mean(img, axis=2)\n",
    "        \n",
    "        # Calculate local variance\n",
    "        local_var = ndimage.generic_filter(gray, np.var, size=3)\n",
    "        \n",
    "        # Normalize and threshold\n",
    "        local_var = (local_var - local_var.min()) / (local_var.max() - local_var.min())\n",
    "        masks[i, 0] = torch.from_numpy(local_var > threshold).float().to(images.device)\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d9d049a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:05.561828Z",
     "iopub.status.busy": "2025-01-11T17:26:05.561595Z",
     "iopub.status.idle": "2025-01-11T17:26:05.565402Z",
     "shell.execute_reply": "2025-01-11T17:26:05.564807Z"
    },
    "papermill": {
     "duration": 0.012052,
     "end_time": "2025-01-11T17:26:05.566700",
     "exception": false,
     "start_time": "2025-01-11T17:26:05.554648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to generate target masks\n",
    "def generate_hiding_spots(images, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Generate target masks indicating good hiding spots.\n",
    "    This is a simple example using edge detection and local variance.\n",
    "    \"\"\"\n",
    "    batch_size = images.size(0)\n",
    "    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n",
    "\n",
    "    # Example implementation: Fill the masks with zeros or apply a heuristic\n",
    "    for i in range(batch_size):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        # Add your heuristic for detecting hiding spots here\n",
    "        # For example, based on edge detection, variance, or other methods\n",
    "        pass\n",
    "\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fea7fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T17:26:05.579901Z",
     "iopub.status.busy": "2025-01-11T17:26:05.579700Z",
     "iopub.status.idle": "2025-01-11T18:49:14.076257Z",
     "shell.execute_reply": "2025-01-11T18:49:14.075277Z"
    },
    "papermill": {
     "duration": 4988.511842,
     "end_time": "2025-01-11T18:49:14.084878",
     "exception": false,
     "start_time": "2025-01-11T17:26:05.573036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.0091\n",
      "Epoch [2/10], Average Loss: 0.0001\n",
      "Epoch [3/10], Average Loss: 0.0000\n",
      "Epoch [4/10], Average Loss: 0.0000\n",
      "Epoch [5/10], Average Loss: 0.0000\n",
      "Epoch [6/10], Average Loss: 0.0000\n",
      "Epoch [7/10], Average Loss: 0.0000\n",
      "Epoch [8/10], Average Loss: 0.0000\n",
      "Epoch [9/10], Average Loss: 0.0000\n",
      "Epoch [10/10], Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64  # Define the batch size for DataLoader\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0  # Track the total loss for the epoch\n",
    "\n",
    "    for i, (images, _) in enumerate(train_loader):  # Process in batches\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Generate target masks\n",
    "        target_masks = generate_hiding_spots(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        location_maps, _ = model(images)  # Unpack the tuple to get location_map\n",
    "\n",
    "        # Upsample location_maps to match target_masks\n",
    "        location_maps_upsampled = torch.nn.functional.interpolate(\n",
    "            location_maps, size=target_masks.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(location_maps_upsampled, target_masks)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate loss for the epoch\n",
    "\n",
    "    # Log average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 11102,
     "sourceId": 15444,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 236239,
     "sourceId": 502445,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1391879,
     "sourceId": 2307647,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2395063,
     "sourceId": 4043617,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2919327,
     "sourceId": 5030375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5099.374417,
   "end_time": "2025-01-11T18:49:16.413170",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-11T17:24:17.038753",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
