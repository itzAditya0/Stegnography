{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":502445,"sourceType":"datasetVersion","datasetId":236239},{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":2307647,"sourceType":"datasetVersion","datasetId":1391879},{"sourceId":4043617,"sourceType":"datasetVersion","datasetId":2395063},{"sourceId":5030375,"sourceType":"datasetVersion","datasetId":2919327}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet-pytorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:44:50.581442Z","iopub.execute_input":"2025-01-13T15:44:50.581747Z","iopub.status.idle":"2025-01-13T15:44:56.936170Z","shell.execute_reply.started":"2025-01-13T15:44:50.581705Z","shell.execute_reply":"2025-01-13T15:44:56.935369Z"}},"outputs":[{"name":"stdout","text":"Collecting efficientnet-pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=156e006602a0dba85e6d91ffdc4349cce94f74a4068e8b44a01d871d3fc53019\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision import datasets, transforms, models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom efficientnet_pytorch import EfficientNet\nimport numpy as np\nfrom scipy import ndimage\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.datasets import FashionMNIST","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:44:56.937211Z","iopub.execute_input":"2025-01-13T15:44:56.937452Z","iopub.status.idle":"2025-01-13T15:45:01.357700Z","shell.execute_reply.started":"2025-01-13T15:44:56.937432Z","shell.execute_reply":"2025-01-13T15:45:01.356939Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define Transformation\n\ndef get_transform():\n    return transforms.Compose([\n        transforms.Resize((128, 128)),  # Resize all images to 128x128\n        transforms.Grayscale(num_output_channels=3),  # Convert grayscale images to 3 channels\n        transforms.ToTensor(),  # Convert images to tensor\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images for 3 channels\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:01.359039Z","iopub.execute_input":"2025-01-13T15:45:01.359425Z","iopub.status.idle":"2025-01-13T15:45:01.367212Z","shell.execute_reply.started":"2025-01-13T15:45:01.359400Z","shell.execute_reply":"2025-01-13T15:45:01.366358Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Example for FashionMNIST dataset\nfrom torchvision.datasets import FashionMNIST\n\nfashion_mnist_train = FashionMNIST(root='data', train=True, download=True, transform=get_transform())\nfashion_mnist_test = FashionMNIST(root='data', train=False, download=True, transform=get_transform())\n\n# You can apply similar code to the other datasets.\n\ntransform = get_transform()\n\n# Load CIFAR-10 dataset\ncifar10_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ncifar10_test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# Load FashionMNIST dataset\nfashionmnist_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\nfashionmnist_test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n\n# Load StegoImages dataset\nfrom torchvision.datasets import ImageFolder\nstego_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/train', transform=transform)\nstego_test_data = ImageFolder(root='/kaggle/input/stegoimagesdataset/test', transform=transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:01.368483Z","iopub.execute_input":"2025-01-13T15:45:01.368747Z","iopub.status.idle":"2025-01-13T15:45:41.845863Z","shell.execute_reply.started":"2025-01-13T15:45:01.368724Z","shell.execute_reply":"2025-01-13T15:45:41.845184Z"}},"outputs":[{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 26421880/26421880 [00:00<00:00, 114014486.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29515/29515 [00:00<00:00, 3989779.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4422102/4422102 [00:00<00:00, 61456930.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5148/5148 [00:00<00:00, 21004160.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:08<00:00, 20835986.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"combined_train_data = ConcatDataset([cifar10_data, fashionmnist_data, stego_data])\ncombined_test_data = ConcatDataset([cifar10_test_data, fashionmnist_test_data, stego_test_data])\n\ntrain_loader = DataLoader(combined_train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(combined_test_data, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:41.846633Z","iopub.execute_input":"2025-01-13T15:45:41.846927Z","iopub.status.idle":"2025-01-13T15:45:41.851582Z","shell.execute_reply.started":"2025-01-13T15:45:41.846897Z","shell.execute_reply":"2025-01-13T15:45:41.850805Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet\n\nclass StegoLocationNet(nn.Module):\n    def __init__(self, num_classes):\n        super(StegoLocationNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Load EfficientNet base model\n        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n\n        # Retrieve the in_features from the original _fc layer\n        if hasattr(self.base_model, '_fc') and isinstance(self.base_model._fc, nn.Linear):\n            in_features = self.base_model._fc.in_features\n        else:\n            raise AttributeError(\"EfficientNet model does not have a valid '_fc' layer.\")\n\n        # Replace the classification layer\n        self.base_model._fc = nn.Identity()  # Remove the existing fully connected layer\n\n        # Add custom classification head\n        self.classifier = nn.Linear(in_features, num_classes)\n\n        # Add location detection layers\n        self.location_head = nn.Sequential(\n            nn.Conv2d(in_features, 128, kernel_size=1),  # Match input channels to 1280\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Extract features before the final classifier\n        features = self.base_model.extract_features(x)\n\n        # Generate location map\n        location_map = self.location_head(features)\n\n        # Optional classification (not used for location-only tasks)\n        pooled_features = torch.mean(features, dim=[2, 3])  # Global average pooling\n        class_output = self.classifier(pooled_features)\n\n        return location_map, class_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:41.852459Z","iopub.execute_input":"2025-01-13T15:45:41.852740Z","iopub.status.idle":"2025-01-13T15:45:41.875855Z","shell.execute_reply.started":"2025-01-13T15:45:41.852711Z","shell.execute_reply":"2025-01-13T15:45:41.875233Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def generate_hiding_spots(images):\n    # Example heuristic: higher values in uniform regions\n    # Apply Sobel filter to find edges\n    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3).float().to(images.device)\n    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3).float().to(images.device)\n\n    edges_x = F.conv2d(images, sobel_x, padding=1)\n    edges_y = F.conv2d(images, sobel_y, padding=1)\n    edge_magnitude = torch.sqrt(edges_x**2 + edges_y**2)\n\n    # Normalize and invert edge magnitude to prioritize smooth regions\n    suitability = 1 - (edge_magnitude / edge_magnitude.max())\n    return suitability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:41.876645Z","iopub.execute_input":"2025-01-13T15:45:41.876911Z","iopub.status.idle":"2025-01-13T15:45:41.893759Z","shell.execute_reply.started":"2025-01-13T15:45:41.876885Z","shell.execute_reply":"2025-01-13T15:45:41.893041Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"num_classes = 10  # Update based on your dataset\nmodel = StegoLocationNet(num_classes=num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:41.894455Z","iopub.execute_input":"2025-01-13T15:45:41.894726Z","iopub.status.idle":"2025-01-13T15:45:42.892764Z","shell.execute_reply.started":"2025-01-13T15:45:41.894700Z","shell.execute_reply":"2025-01-13T15:45:42.891931Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n100%|██████████| 20.4M/20.4M [00:00<00:00, 95.5MB/s]","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n# Get the number of classes from your dataset\nnum_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n\n# Load the EfficientNet model\n# Get the number of classes from your dataset\nnum_classes = len(cifar10_data.classes) + len(fashionmnist_data.classes) + len(stego_data.classes)\n\n# Create model\nmodel = StegoLocationNet(num_classes=num_classes)\n\n# Move model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:42.894816Z","iopub.execute_input":"2025-01-13T15:45:42.895051Z","iopub.status.idle":"2025-01-13T15:45:43.300641Z","shell.execute_reply.started":"2025-01-13T15:45:42.895031Z","shell.execute_reply":"2025-01-13T15:45:43.298856Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:43.301920Z","iopub.execute_input":"2025-01-13T15:45:43.302245Z","iopub.status.idle":"2025-01-13T15:45:43.307793Z","shell.execute_reply.started":"2025-01-13T15:45:43.302211Z","shell.execute_reply":"2025-01-13T15:45:43.306820Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Add this helper function above the training loop\ndef generate_hiding_spots(images, threshold=0.8):\n    \"\"\"\n    Generate target masks indicating good hiding spots\n    This is a simple example using edge detection and local variance\n    \"\"\"\n    batch_size = images.size(0)\n    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n    \n    for i in range(batch_size):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        \n        # Convert to grayscale\n        gray = np.mean(img, axis=2)\n        \n        # Calculate local variance\n        local_var = ndimage.generic_filter(gray, np.var, size=3)\n        \n        # Normalize and threshold\n        local_var = (local_var - local_var.min()) / (local_var.max() - local_var.min())\n        masks[i, 0] = torch.from_numpy(local_var > threshold).float().to(images.device)\n    \n    return masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:43.308806Z","iopub.execute_input":"2025-01-13T15:45:43.309107Z","iopub.status.idle":"2025-01-13T15:45:43.330687Z","shell.execute_reply.started":"2025-01-13T15:45:43.309078Z","shell.execute_reply":"2025-01-13T15:45:43.329839Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define the function to generate target masks\ndef generate_hiding_spots(images, threshold=0.8):\n    \"\"\"\n    Generate target masks indicating good hiding spots.\n    This is a simple example using edge detection and local variance.\n    \"\"\"\n    batch_size = images.size(0)\n    masks = torch.zeros((batch_size, 1, images.size(2), images.size(3))).to(images.device)\n\n    # Example implementation: Fill the masks with zeros or apply a heuristic\n    for i in range(batch_size):\n        img = images[i].cpu().permute(1, 2, 0).numpy()\n        # Add your heuristic for detecting hiding spots here\n        # For example, based on edge detection, variance, or other methods\n        pass\n\n    return masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:43.331531Z","iopub.execute_input":"2025-01-13T15:45:43.331744Z","iopub.status.idle":"2025-01-13T15:45:43.350100Z","shell.execute_reply.started":"2025-01-13T15:45:43.331726Z","shell.execute_reply":"2025-01-13T15:45:43.349248Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 2\nbatch_size = 64  # Define the batch size for DataLoader\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0  # Track the total loss for the epoch\n\n    for i, (images, _) in enumerate(train_loader):  # Process in batches\n        images = images.to(device)\n\n        # Generate target masks\n        target_masks = generate_hiding_spots(images)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        location_maps, _ = model(images)  # Unpack the tuple to get location_map\n\n        # Upsample location_maps to match target_masks\n        location_maps_upsampled = torch.nn.functional.interpolate(\n            location_maps, size=target_masks.shape[2:], mode='bilinear', align_corners=False\n        )\n\n        # Calculate loss\n        loss = criterion(location_maps_upsampled, target_masks)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()  # Accumulate loss for the epoch\n\n    # Log average loss for the epoch\n    avg_loss = epoch_loss / len(train_loader)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T15:45:43.351036Z","iopub.execute_input":"2025-01-13T15:45:43.351329Z","iopub.status.idle":"2025-01-13T16:02:12.019522Z","shell.execute_reply.started":"2025-01-13T15:45:43.351260Z","shell.execute_reply":"2025-01-13T16:02:12.018755Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/2], Average Loss: 0.0092\nEpoch [2/2], Average Loss: 0.0001\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(f\"Location maps size: {location_maps.shape}\")\nprint(f\"Target masks size: {target_masks.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T16:02:12.020318Z","iopub.execute_input":"2025-01-13T16:02:12.020550Z","iopub.status.idle":"2025-01-13T16:02:12.025390Z","shell.execute_reply.started":"2025-01-13T16:02:12.020530Z","shell.execute_reply":"2025-01-13T16:02:12.024559Z"}},"outputs":[{"name":"stdout","text":"Location maps size: torch.Size([48, 1, 4, 4])\nTarget masks size: torch.Size([48, 1, 128, 128])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0.0\n    with torch.no_grad():\n        for images, _ in test_loader:  # Ignore original labels\n            images = images.to(device)\n\n            # Generate target masks for testing\n            target_masks = generate_hiding_spots(images)\n\n            # Forward pass\n            location_maps, _ = model(images)\n\n            # Resize `location_maps` to match `target_masks` size\n            location_maps_resized = F.interpolate(\n                location_maps, size=target_masks.shape[2:], mode=\"bilinear\", align_corners=False\n            )\n\n            # Compute loss\n            loss = criterion(location_maps_resized, target_masks)\n\n            # Accumulate loss\n            test_loss += loss.item()\n\n    avg_test_loss = test_loss / len(test_loader)\n    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n    return avg_test_loss\n\n# Perform evaluation\naverage_test_loss = evaluate_model(model, test_loader, criterion, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T16:02:12.026099Z","iopub.execute_input":"2025-01-13T16:02:12.026338Z","iopub.status.idle":"2025-01-13T16:05:58.890165Z","shell.execute_reply.started":"2025-01-13T16:02:12.026317Z","shell.execute_reply":"2025-01-13T16:05:58.889366Z"}},"outputs":[{"name":"stdout","text":"Average Test Loss: 0.0000\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Save the trained model\ntorch.save(model.state_dict(), 'stegonet.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T16:19:17.446929Z","iopub.execute_input":"2025-01-13T16:19:17.447345Z","iopub.status.idle":"2025-01-13T16:19:17.512318Z","shell.execute_reply.started":"2025-01-13T16:19:17.447310Z","shell.execute_reply":"2025-01-13T16:19:17.511370Z"}},"outputs":[],"execution_count":17}]}